{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN Model code which require google drive access for dataset"
      ],
      "metadata": {
        "id": "wHhoqwU8m9d-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJQ8wAvUlJLf",
        "outputId": "b1068a7e-4573-4774-b19d-7d6ffd92d86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "29/29 [==============================] - 6s 116ms/step - loss: 0.6656 - accuracy: 0.6243 - val_loss: 0.5918 - val_accuracy: 0.5844\n",
            "Epoch 2/10\n",
            "29/29 [==============================] - 3s 90ms/step - loss: 0.5077 - accuracy: 0.7752 - val_loss: 0.5855 - val_accuracy: 0.7489\n",
            "Epoch 3/10\n",
            "29/29 [==============================] - 3s 93ms/step - loss: 0.3803 - accuracy: 0.8187 - val_loss: 0.3427 - val_accuracy: 0.8355\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - 3s 88ms/step - loss: 0.3979 - accuracy: 0.8306 - val_loss: 0.3504 - val_accuracy: 0.7965\n",
            "Epoch 5/10\n",
            "29/29 [==============================] - 3s 90ms/step - loss: 0.2902 - accuracy: 0.8643 - val_loss: 0.2369 - val_accuracy: 0.9048\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - 3s 95ms/step - loss: 0.2488 - accuracy: 0.8914 - val_loss: 0.2085 - val_accuracy: 0.9048\n",
            "Epoch 7/10\n",
            "29/29 [==============================] - 3s 92ms/step - loss: 0.2161 - accuracy: 0.9077 - val_loss: 0.1261 - val_accuracy: 0.9264\n",
            "Epoch 8/10\n",
            "29/29 [==============================] - 3s 95ms/step - loss: 0.1701 - accuracy: 0.9273 - val_loss: 0.1378 - val_accuracy: 0.9394\n",
            "Epoch 9/10\n",
            "29/29 [==============================] - 3s 94ms/step - loss: 0.1389 - accuracy: 0.9435 - val_loss: 0.1204 - val_accuracy: 0.9481\n",
            "Epoch 10/10\n",
            "29/29 [==============================] - 3s 89ms/step - loss: 0.1070 - accuracy: 0.9555 - val_loss: 0.1577 - val_accuracy: 0.8874\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.2178 - accuracy: 0.8993\n",
            "Test accuracy: 0.8993055820465088\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '13wgU2AtpPQvyzJKQNWX5NUIhnq391BOR'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('data.zip')\n",
        "\n",
        "with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')\n",
        "\n",
        "def load_images(directory, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(directory):\n",
        "        img_path = os.path.join(directory, filename)\n",
        "        if os.path.isfile(img_path):\n",
        "            img = load_img(img_path, target_size=(height, width))\n",
        "            img_array = img_to_array(img)\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "height, width = 255, 255\n",
        "channels = 3\n",
        "\n",
        "bad_images, bad_labels = load_images('/content/data/output/bad', label=0)\n",
        "good_images, good_labels = load_images('/content/data/output/good', label=1)\n",
        "\n",
        "all_images = np.concatenate([bad_images, good_images], axis=0)\n",
        "all_labels = np.concatenate([bad_labels, good_labels], axis=0)\n",
        "\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    all_images, all_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(height, width, channels)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dense(128,activation='relu'))\n",
        "model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "adam_optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=adam_optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "train_labels_one_hot = to_categorical(train_labels)\n",
        "test_labels_one_hot = to_categorical(test_labels)\n",
        "\n",
        "model.fit(train_images, train_labels_one_hot, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels_one_hot)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for manual testing"
      ],
      "metadata": {
        "id": "6-5yzXHgoTrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# file_id = '17ReCiLc6NsrtOUhG4qGkbWjDmid9xZDM'\n",
        "# downloaded = drive.CreateFile({'id': file_id})\n",
        "# downloaded.GetContentFile('Good.zip')\n",
        "\n",
        "# file_id = '1XBdO6dM8IyNQ6-HUGGwRCPQajxx30Xtz'\n",
        "# downloaded = drive.CreateFile({'id': file_id})\n",
        "# downloaded.GetContentFile('Bad.zip')\n",
        "\n",
        "# with zipfile.ZipFile('Good.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('Good')\n",
        "\n",
        "# test_folder_path = '/content/Good/Good/'\n",
        "\n",
        "with zipfile.ZipFile('Bad.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('Bad')\n",
        "\n",
        "test_folder_path = '/content/Bad/Bad/'\n",
        "\n",
        "for image_name in os.listdir(test_folder_path):\n",
        "    image_path = os.path.join(test_folder_path, image_name)\n",
        "\n",
        "    img = load_img(image_path, target_size=(height, width))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "\n",
        "    if prediction[0][0] > 0.5:\n",
        "        print(prediction[0][0])\n",
        "        print(f\"Image {image_name}: Predicted as Good\")\n",
        "    else:\n",
        "        print(prediction[0][0])\n",
        "        print(f\"Image {image_name}: Predicted as Bad\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Voe7tXjPlUKv",
        "outputId": "a8d4950d-c6bb-4c43-fdad-e37a5bd2c993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "0.9946956\n",
            "Image BAD_21.png: Predicted as Good\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "0.025447687\n",
            "Image BAD_145.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "6.209581e-08\n",
            "Image BAD_178.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "0.0023283006\n",
            "Image BAD_176.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.31944823\n",
            "Image 4k_3 4.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.0005550079\n",
            "Image 5k_2 1.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "0.14401662\n",
            "Image 4k_3 5.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "0.3946886\n",
            "Image BAD_134.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "2.3653334e-13\n",
            "Image BAD_179.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "0.7574769\n",
            "Image BAD_140.png: Predicted as Good\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.3315168\n",
            "Image BAD_148.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "0.9999162\n",
            "Image 20k_2 7.png: Predicted as Good\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.0001311568\n",
            "Image 7k_2 1.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.00071896747\n",
            "Image BAD_168.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "7.94815e-18\n",
            "Image BAD_169.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.36744896\n",
            "Image BAD_133.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "0.036919586\n",
            "Image BAD_150.png: Predicted as Bad\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "0.9469109\n",
            "Image 19k_3 5.png: Predicted as Good\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "0.9162925\n",
            "Image 4k_3 3.png: Predicted as Good\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1.3719252e-16\n",
            "Image BAD_174.png: Predicted as Bad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "folder_path = '/content/test/'\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\"Folder '{folder_path}' and its contents have been deleted.\")\n",
        "else:\n",
        "    print(f\"Folder '{folder_path}' does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irw7yyKL2P-Y",
        "outputId": "72b3c541-088a-4e8c-9daf-bc4b44b70352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/test/' and its contents have been deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STFT generation code"
      ],
      "metadata": {
        "id": "fBinc6MDodI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import os\n",
        "\n",
        "def calcSTFT_norm(df, column_name, samplingFreq=10000, window='hann', nperseg=100000, figsize=(10,6), cmap='viridis', ylim_max=None, save_path=None):\n",
        "    inputSignal = df[column_name].values\n",
        "\n",
        "    if len(inputSignal) < nperseg:\n",
        "        print(f\"Warning: nperseg = {nperseg} is greater than input length = {len(inputSignal)}. Skipping STFT calculation.\")\n",
        "        return\n",
        "\n",
        "    num_chunks = len(inputSignal) // nperseg\n",
        "    remainder = len(inputSignal) % nperseg\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_idx = i * nperseg\n",
        "        end_idx = (i + 1) * nperseg\n",
        "        chunk_signal = inputSignal[start_idx:end_idx]\n",
        "\n",
        "        f, t, Zxx = signal.stft(chunk_signal, samplingFreq, window=window, nperseg=nperseg)\n",
        "\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        spec = plt.pcolormesh(t, f, np.abs(Zxx),\n",
        "                              norm=colors.PowerNorm(gamma=1./6.),\n",
        "                              cmap=plt.get_cmap(cmap))\n",
        "        cbar = plt.colorbar(spec)\n",
        "\n",
        "        ax = fig.axes[0]\n",
        "        ax.set_xlim(0, t[-1])\n",
        "\n",
        "        plt.title(f'STFT Spectrogram for {column_name} - Chunk {i+1}')\n",
        "        ax.grid(True)\n",
        "        ax.set_title('STFT Magnitude')\n",
        "        if ylim_max:\n",
        "            ax.set_ylim(0, ylim_max)\n",
        "        ax.set_ylabel('Frequency [Hz]')\n",
        "        ax.set_xlabel('Time [sec]')\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(f'{save_path[:-4]} {i+1}.png')\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    if remainder > 0:\n",
        "        remaining_signal = inputSignal[-remainder:]\n",
        "        f, t, Zxx = signal.stft(remaining_signal, samplingFreq, window=window, nperseg=nperseg)\n",
        "\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        spec = plt.pcolormesh(t, f, np.abs(Zxx),\n",
        "                              norm=colors.PowerNorm(gamma=1./6.),\n",
        "                              cmap=plt.get_cmap(cmap))\n",
        "        cbar = plt.colorbar(spec)\n",
        "\n",
        "        ax = fig.axes[0]\n",
        "        ax.set_xlim(0, t[-1])\n",
        "\n",
        "        plt.title(f'STFT Spectrogram for {column_name} - Remaining Data')\n",
        "        ax.grid(True)\n",
        "        ax.set_title('STFT Magnitude')\n",
        "        if ylim_max:\n",
        "            ax.set_ylim(0, ylim_max)\n",
        "        ax.set_ylabel('Frequency [Hz]')\n",
        "        ax.set_xlabel('Time [sec]')\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(f'{save_path[:-4]}_remaining.png')\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "df = pd.read_csv('/test.csv')\n",
        "\n",
        "\n",
        "column_names = ['cDAQ9189-20796A8Mod1_ai1']\n",
        "\n",
        "output_directory = r'P:\\pr\\Internship\\test_op'\n",
        "\n",
        "for i, column_name in enumerate(column_names, start=1):\n",
        "    base_save_path = os.path.join(output_directory, f'test_{i}')\n",
        "\n",
        "    save_path = f'{base_save_path}.png'\n",
        "    calcSTFT_norm(df, column_name, save_path=save_path)\n"
      ],
      "metadata": {
        "id": "yBPN4ioLoPlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing code - Resize"
      ],
      "metadata": {
        "id": "NGBo0CRQnPIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def crop_image(input_path, output_path, left, top, width, height):\n",
        "    img = Image.open(input_path)\n",
        "\n",
        "    right = left + width\n",
        "    bottom = top + height\n",
        "\n",
        "    cropped_img = img.crop((left, top, right, bottom))\n",
        "\n",
        "    cropped_img.save(output_path)\n",
        "\n",
        "def batch_crop_images(input_folder, output_folder, crop_info):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "            crop_image(input_path, output_path, *crop_info)\n",
        "\n",
        "input_folder = \"P:\\\\pr\\\\Internship\\\\output\\\\good\"\n",
        "output_folder = \"P:\\\\pr\\\\Internship\\\\output\\\\good\"\n",
        "crop_info = (123, 71, 685, 466)  # (left, top, width, height)\n",
        "# 9:5  image dimensions crop_info = (112, 59, 615, 389)  # (left, top, width, height)\n",
        "# 10:6 image dimensions crop_info = (123, 71, 685, 466)  # (left, top, width, height)\n",
        "\n",
        "batch_crop_images(input_folder, output_folder,crop_info)"
      ],
      "metadata": {
        "id": "mthY80THm9z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing -"
      ],
      "metadata": {
        "id": "1MABb3G3nVKl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09tEHKl3nU5m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}